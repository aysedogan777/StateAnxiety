{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa39184e",
   "metadata": {},
   "source": [
    "# Ayse Dogan\n",
    "## Creating splits for 2-weeks datasets\n",
    "### 02/20/25 - Project3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2483b7-48f0-4495-81a8-dd884fd73156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predefined the file names for e4 and hex\n",
    "#list(hex_data_dict[33]['day0_hex']['csv'].keys())+list(hex_data_dict[33]['day0_hex']['wav'].keys())\n",
    "e4_file_names = ['TEMP.csv', 'tags.csv', 'HR.csv', 'ACC.csv', 'IBI.csv', 'EDA.csv', 'BVP.csv']\n",
    "hex_file_names = ['RR_interval_quality.csv', 'NN_interval.csv', 'statistics.csv', 'sleep_position.csv', 'PTT.csv', 'step.csv', 'inspiration.csv', 'RR_interval.csv', 'expiration.csv', \n",
    "                  'device_position.csv', 'heart_rate.wav', 'heart_rate_quality.wav', 'systolic_pressure.wav', 'tidal_volume_adjusted.wav', 'minute_ventilation.wav', 'tidal_volume.wav', \n",
    "                  'respiration_abdominal.wav', 'acceleration_X.wav', 'acceleration_Y.wav', 'respiration_thoracic.wav', 'acceleration_Z.wav', 'ECG_I.wav', 'minute_ventilation_adjusted.wav', \n",
    "                  'ECG_III.wav', 'energy_mifflin_keytel.wav', 'systolic_pressure_adjusted.wav', 'breathing_rate_quality.wav', 'ECG_II.wav', 'cadence.wav', 'breathing_rate.wav', 'activity.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4fe3c-3e02-4c72-a853-c49e514356ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the Datasets\n",
    "#parent_path = '/Users/adogan/Library/CloudStorage/Box-Box/[Box Health - Internal] Rad-Wear/[Box Health - Internal] WEAR data collection/WEAR Observation Peroid'\n",
    "parent_path = '/work/hdd/bbnp/wear_raw/WEAR Observation Peroid'\n",
    "participant_list = os.listdir(parent_path)\n",
    "# Filter the list for entries that start with 'WEAR' and do not end with 'alias'\n",
    "filtered_participant_list = [name for name in participant_list if name.startswith('WEAR') and not name.endswith('alias')]\n",
    "print(\"The number of participants is: {}\".format(len(filtered_participant_list)))\n",
    "#filtered_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16933f-d866-4776-a2fa-0b584da3e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 0\n",
    "def get_participant_id(order):\n",
    "    participant_id = int(filtered_participant_list[order][filtered_participant_list[order].index(' ')+2:])\n",
    "    return(participant_id)\n",
    "\n",
    "def generate_participant_id_list():\n",
    "    participant_id_list = []\n",
    "    for i in range(len(filtered_participant_list)):\n",
    "        id = get_participant_id(i)\n",
    "        participant_id_list.append(id)\n",
    "    return(participant_id_list)\n",
    "participant_id_list = generate_participant_id_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058da25",
   "metadata": {},
   "source": [
    "## Get the participant informations from the tracking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a2fd7-1105-471e-bcd6-66c64093fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Excel file\n",
    "excel_file_path = '/Users/adogan/Library/CloudStorage/Box-Box/[Box Health - Internal] Rad-Wear/[Box Health - Internal] WEAR data collection/WEAR participant tracking.xlsx'\n",
    "excel_file_path = '/work/hdd/bbnp/wear_raw/WEAR participant tracking.xlsx'\n",
    "all_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "\n",
    "# The dictionary now contains sheet names as keys and their DataFrames as values\n",
    "all_sheets_keys = all_sheets.keys()  # Display all sheet names\n",
    "participants_sheets_keys = [name for name in all_sheets_keys if name.startswith('P') and not name.startswith('Participants') ]\n",
    "#participants_sheets_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ad8b5-b690-4d54-849a-5439a74c9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with only the filtered keys\n",
    "participants_sheets_dict = {\n",
    "    name: all_sheets[name]  # Keep the original key-value pair\n",
    "    for name in participants_sheets_keys  # Use your filtered list of keys\n",
    "}\n",
    "\n",
    "# Rename keys by extracting integers after 'P'\n",
    "participants_sheets_dict = {\n",
    "    int(name[1:]): participants_sheets_dict[name]  # Extract the integer after 'P' and set it as the key\n",
    "    for name in participants_sheets_keys  # Use your filtered sheet names list\n",
    "}\n",
    "\n",
    "# Display the new dictionary keys\n",
    "print(\"The number of participants in the tracking list: {}\".format(len(set(list(participants_sheets_dict.keys())))))\n",
    "print(participants_sheets_dict.keys())\n",
    "\n",
    "#unwanted_numbers = {2, 223, 249, 252, 253, 264, 268, 290} #why we subtracted them I have no idea #2/20/25\n",
    "wanted_numbers = set(list(participants_sheets_dict.keys())) & set(participant_id_list)\n",
    "# Filter the participant list to exclude matching numbers\n",
    "filtered_participant_list = [\n",
    "    name for name in filtered_participant_list\n",
    "    if name.split(' ')[-1][1:].isdigit() and int(name.split(' ')[-1][1:]) in wanted_numbers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d912655-f596-4118-963a-c911a264ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the paths for the patients\n",
    "def get_the_paths_to_patient_folders(order, parent_path = parent_path, filtered_participant_list =filtered_participant_list):\n",
    "    participant_folder = parent_path+'/'+filtered_participant_list[order]\n",
    "    folder_list_of_patient = os.listdir(participant_folder)\n",
    "    folder_list_of_patient = [folder for folder in folder_list_of_patient if folder != '.DS_Store']\n",
    "    return(participant_folder)\n",
    "\n",
    "# get_the_paths_to_patient_folders(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be451b-eb59-4e67-8299-3d3270d75f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the participants data in the recorded excels\n",
    "def get_device_day_details(order,participants_sheets_dict=participants_sheets_dict):\n",
    "    # Access the DataFrame for the specific participant\n",
    "    #print('participant_id:', get_participant_id(order))\n",
    "    participant_patients_dayly_tags = participants_sheets_dict[get_participant_id(order)]\n",
    "    \n",
    "    # Set the 3rd row (index 2) as the header\n",
    "    participant_patients_dayly_tags.columns = participant_patients_dayly_tags.iloc[3,:].tolist()  # Set the 3rd row as column headers\n",
    "    participant_patients_dayly_tags = participant_patients_dayly_tags[4:]  # Remove the first 3 rows to clean the data\n",
    "    \n",
    "    # Reset the index for clean DataFrame\n",
    "    participant_patients_dayly_tags.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return participant_patients_dayly_tags\n",
    "participant_patients_dayly_tags = get_device_day_details(order)\n",
    "#participant_patients_dayly_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd14b3-dabf-442f-b6c7-fc141acacd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name each day to create a map\n",
    "def generate_dataset_dict(order):\n",
    "    participant_patients_dayly_tags = get_device_day_details(order)\n",
    "    # Initialize an empty dictionary\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Iterate through the rows of the DataFrame\n",
    "    for index, row in participant_patients_dayly_tags.iterrows():\n",
    "        # Format the keys\n",
    "        e4_key = f\"day{index:1}_e4\"  # Key for E4 (e.g., day00_e4)\n",
    "        hex_key = f\"day{index:1}_hex\"  # Key for Hexoskin (e.g., day00_hex)\n",
    "        \n",
    "        # Assign values from the DataFrame to the dictionary\n",
    "        data_dict[e4_key] = row[participant_patients_dayly_tags.columns[1]]  # Value from the E4 column\n",
    "        data_dict[hex_key] = row[participant_patients_dayly_tags.columns[2]]  # Value from the Hexoskin column\n",
    "    \n",
    "    # Print the resulting dictionary\n",
    "    return(data_dict)\n",
    "\n",
    "participant_folder_list = generate_dataset_dict(order)\n",
    "#participant_folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read each participant base ond [key][day] ==> [order][file_code]\n",
    "\n",
    "datasets = ['ECG_I.wav','breathing_rate.wav','acceleration_X.wav','acceleration_Y.wav','acceleration_Z.wav', ]\n",
    "def generate_selected_dataset(hex_data_dict,datasets = datasets):\n",
    "    e4_BVP_dict = {}\n",
    "    for key, directory in hex_data_dict.items():\n",
    "        e4_BVP_dict[key] = {}\n",
    "        for day, read_files in directory.items():\n",
    "            e4_BVP_dict[key][day] = {}\n",
    "            read_files_list = list(read_files)\n",
    "            #print(\"read_files_list:\", read_files_list)\n",
    "            for dataset in datasets:\n",
    "                if dataset in read_files_list:\n",
    "                    # print('key:', key, 'day:', day, 'is tagged.')\n",
    "                    e4_BVP_dict[key][day][dataset] = hex_data_dict[key][day][dataset]\n",
    "                else:\n",
    "                    print('key:', key, 'day:', day, 'is empty.')\n",
    "                    e4_BVP_dict[key][day][dataset] = {}\n",
    "    return(e4_BVP_dict)\n",
    "\n",
    "from scipy.io import wavfile\n",
    "def generate_data_dict_for_wav_files(order, file_code = 'day0_hex', csv_files =datasets):\n",
    "    participant_folder = get_the_paths_to_patient_folders(order)\n",
    "    participant_folder_list = generate_dataset_dict(order)\n",
    "    target = participant_folder+'/'+participant_folder_list[file_code]\n",
    "    # Get the list of files in the directory\n",
    "    #file_list_of_patient_day_device = os.listdir(target)\n",
    "    \n",
    "    # Filter to include only CSV files\n",
    "    #csv_files = [file for file in file_list_of_patient_day_device if file.endswith('.wav')]\n",
    "    #print('csv_files:', csv_files)\n",
    "    \n",
    "    # Create a dictionary to store the data from CSV files\n",
    "    csv_data_dict = {}\n",
    "    \n",
    "    # Loop through the CSV files and read them into the dictionary\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(target, csv_file)  # Construct full path to the CSV file\n",
    "        print('csv_path: ', csv_path)\n",
    "        try:\n",
    "            # Check if the file is empty\n",
    "            if os.stat(csv_path).st_size > 0:  # File is non-empty\n",
    "                sample_rate, data = wavfile.read(csv_path)\n",
    "                #print('sample_rate:',sample_rate, 'for file: ', csv_file)\n",
    "                csv_data_dict[csv_file] = data\n",
    "            else:\n",
    "                print(f\"Skipping empty file: {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "    \n",
    "    # Display the keys of the dictionary (file names) and optionally the first few rows of one DataFrame\n",
    "    #print(csv_data_dict.keys())  # Keys will be the file names (e.g., 'TEMP.csv', 'EDA.csv')\n",
    "    return(csv_data_dict)\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def generate_data_dict_for_wav_files(order,\n",
    "                                     file_code='day0_hex',\n",
    "                                     wav_names=datasets):   # list of expected files\n",
    "    \"\"\"\n",
    "    Return {wav_name: np.ndarray} for every existing, non‑empty WAV in wav_names.\n",
    "    Missing or empty files are skipped with a message.\n",
    "    \"\"\"\n",
    "    # ------- build the day directory path safely -----------------------------\n",
    "    base_dir   = Path(get_the_paths_to_patient_folders(order))\n",
    "    day_folder = base_dir / generate_dataset_dict(order)[file_code]\n",
    "\n",
    "    if not day_folder.is_dir():\n",
    "        #print(f\"[missing folder] {order}\")\n",
    "        return {}\n",
    "\n",
    "    wav_dict = {}\n",
    "\n",
    "    for name in wav_names:\n",
    "        path = day_folder / name                # Path object ⇒ OS‑safe join\n",
    "\n",
    "        # ---------- new existence check --------------------------------------\n",
    "        if not path.is_file():                  # False for missing or a directory\n",
    "           # print(f\"[missing file] {name}\")\n",
    "            continue\n",
    "\n",
    "        # ---------- size check ------------------------------------------------\n",
    "        if path.stat().st_size == 0:\n",
    "            #print(f\"[empty file]   {name}\")\n",
    "            continue\n",
    "\n",
    "        # ---------- safe read -------------------------------------------------\n",
    "        try:\n",
    "            sr, data = wavfile.read(path)\n",
    "            wav_dict[name] = data\n",
    "        except Exception as e:\n",
    "            print(f\"[error reading] {name}: {e}\")\n",
    "\n",
    "    return wav_dict\n",
    "\n",
    "# record all the e4 readings in e4_data_dict: e4_data_dict[id][dayX_e4][<e4 file name>]\n",
    "def hex_data_reading():\n",
    "    hex_data_dict = {}\n",
    "    for order in range(len(filtered_participant_list)):\n",
    "    #for order in range(3):\n",
    "        # Process each 'e4' key\n",
    "        participant_folder_list = generate_dataset_dict(order)\n",
    "        # Get keys ending with 'e4' and 'hex'\n",
    "        #e4_keys = [key for key in participant_folder_list.keys() if key.endswith('e4')]\n",
    "        hex_keys = [key for key in participant_folder_list.keys() if key.endswith('hex')]\n",
    "        key = get_participant_id(order)\n",
    "        #print('key: ', key)\n",
    "        hex_data_dict[key]= {}\n",
    "        for dayX_e4 in hex_keys:\n",
    "            try:\n",
    "                # Generate the data for the current 'e4' key\n",
    "                #hex_data_dict[key][dayX_e4] = generate_data_dict(order, file_code=dayX_e4)\n",
    "                #print('Participant',key, dayX_e4)\n",
    "                hex_data_dict[key][dayX_e4] = generate_data_dict_for_wav_files(order, file_code=dayX_e4)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {dayX_e4} for participant {key}\")\n",
    "                #hex_data_dict[key][dayX_e4] = {}\n",
    "\n",
    "    # # Display the keys and optionally inspect the data\n",
    "    # print(e4_data_dict.keys())\n",
    "    # # Example: print data for a specific key\n",
    "    # if 'day0_e4' in e4_data_dict:\n",
    "    #     print(e4_data_dict['day0_e4'])\n",
    "    return hex_data_dict\n",
    "\n",
    "\n",
    "#hex_selected_dict = hex_data_reading()#hex_data_dict, datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694045b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_daily_data_of_participant(hex_selected_dict, id = 92, day='day0_hex'):\n",
    "    # Assuming e4_BVP_dict[33] is a dictionary where keys are days and values are DataFrames or Series\n",
    "    participant_data = hex_selected_dict[id][day]['ECG_I.wav']  # Replace 33 with the key of the participant you want to analyze\n",
    "\n",
    "    # Loop through each day in the participant's data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # for day, data in participant_data.items():\n",
    "    #     print('day', day)\n",
    "    #     print('data:', data)\n",
    "    #     if len(data) == 0:\n",
    "    #         print(\"{day} is empty!\")\n",
    "    #     else:\n",
    "    #         #plt.figure(figsize=(12, 6))\n",
    "            \n",
    "    #         # Plot the data for the day\n",
    "    plt.plot(participant_data, color='blue', linewidth=0.5)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(f'Data for participant {id} {day}', fontsize=16)\n",
    "    plt.xlabel('Time (samples)', fontsize=14)\n",
    "    plt.ylabel('BVP Value', fontsize=14)\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97143d2c",
   "metadata": {},
   "source": [
    "# E4 Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record all the e4 readings in e4_data_dict: e4_data_dict[id][dayX_e4][<e4 file name>]\n",
    "#Read each participant base ond [key][day] ==> [order][file_code]\n",
    "def generate_data_dict(order, file_code = 'day0_e4', file_type = 'TEMP'):\n",
    "    participant_folder = get_the_paths_to_patient_folders(order)\n",
    "    participant_folder_list = generate_dataset_dict(order)\n",
    "    target = participant_folder+'/'+participant_folder_list[file_code]\n",
    "    # Get the list of files in the directory\n",
    "    file_list_of_patient_day_device = os.listdir(target)\n",
    "    \n",
    "    # Filter to include only CSV files\n",
    "    \n",
    "    if file_type == 'TEMP':\n",
    "        csv_files = [file for file in file_list_of_patient_day_device if file.startswith('TEMP.csv') or file.startswith('tags.csv')]\n",
    "    else:\n",
    "        csv_files = [file for file in file_list_of_patient_day_device if file.endswith('.csv')]\n",
    "    # Create a dictionary to store the data from CSV files\n",
    "    csv_data_dict = {}\n",
    "    \n",
    "    # Loop through the CSV files and read them into the dictionary\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(target, csv_file)  # Construct full path to the CSV file\n",
    "        try:\n",
    "            # Check if the file is empty\n",
    "            if os.stat(csv_path).st_size > 0:  # File is non-empty\n",
    "                csv_data_dict[csv_file] = pd.read_csv(csv_path)\n",
    "            else:\n",
    "                print(f\"Skipping empty file: {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "    \n",
    "    # Display the keys of the dictionary (file names) and optionally the first few rows of one DataFrame\n",
    "    #print(csv_data_dict.keys())  # Keys will be the file names (e.g., 'TEMP.csv', 'EDA.csv')\n",
    "    return(csv_data_dict)\n",
    "\n",
    "\n",
    "def e4_data_reading(file_type = 'TEMP'):\n",
    "    e4_data_dict = {}\n",
    "    for order in range(len(filtered_participant_list)):\n",
    "    #for order in range(3):\n",
    "        # Process each 'e4' key\n",
    "        participant_folder_list = generate_dataset_dict(order)\n",
    "        # Get keys ending with 'e4' and 'hex'\n",
    "        e4_keys = [key for key in participant_folder_list.keys() if key.endswith('e4')]\n",
    "        # hex_keys = [key for key in participant_folder_list.keys() if key.endswith('hex')]\n",
    "        key = get_participant_id(order)\n",
    "        #print('key: ', key)\n",
    "        e4_data_dict[key]= {}\n",
    "        for dayX_e4 in e4_keys:\n",
    "            try:\n",
    "                # Generate the data for the current 'e4' key\n",
    "                e4_data_dict[key][dayX_e4] = generate_data_dict(order, file_code=dayX_e4,file_type = file_type)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {dayX_e4} for participant {key}\")\n",
    "                e4_data_dict[key][dayX_e4] = {}\n",
    "\n",
    "    # # Display the keys and optionally inspect the data\n",
    "    # print(e4_data_dict.keys())\n",
    "    # # Example: print data for a specific key\n",
    "    # if 'day0_e4' in e4_data_dict:\n",
    "    #     print(e4_data_dict['day0_e4'])\n",
    "    return e4_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94357ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_column(beginning_time,selected_dict_to_add_column , fs_ecg =256,fs_br = 1,fs_acc = 64):\n",
    "    #participant_dict_HR ={}\n",
    "    #day_num = day[3]\n",
    "\n",
    "    '''ECG processing'''\n",
    "    #raw_ECG = hex_selected_dict[participant][day]['ECG_I.wav']\n",
    "    raw_ECG = selected_dict_to_add_column['ECG_I.wav']\n",
    "    raw_ECG = pd.DataFrame(data = raw_ECG)\n",
    "    ecg = 0.0064 * raw_ECG #get correct magnitude of ECG\n",
    "    ecg.rename(columns = {0: 'ECG'}, inplace = True)\n",
    "    #date_info = e4_data_dict[participant]['day'+day_num+'_e4']['TEMP.csv'].columns[0]\n",
    "    #ecg\n",
    "    t0_ecg = beginning_time\n",
    "    #ecg = participant_info_hex['raw_ECG']\n",
    "\n",
    "    ecg['Timestamp'] = list(range(0, ecg.shape[0],1))\n",
    "    ecg['Timestamp'] = ecg['Timestamp'].apply(lambda x: x/fs_ecg+t0_ecg)\n",
    "    #ecg['Timestamp'] = ecg['Timestamp'].str.get(0)\n",
    "    # ecg['Second'] = ecg['Timestamp']\n",
    "    ecg = ecg.set_index('Timestamp')\n",
    "    # ecg['Second'] = ecg['Second'].apply(lambda x: x-ecg.index[0])\n",
    "    ecg = ecg.reset_index()\n",
    "    ecg.rename(columns={0: 'ecg'}, inplace=True)\n",
    "\n",
    "\n",
    "    '''BR processing'''\n",
    "    #raw_br = hex_selected_dict[participant][day]['breathing_rate.wav'] # wavfile.read(os.path.join(ecg_folder, subfolder, \"breathing_rate.wav\"))\n",
    "    raw_br = selected_dict_to_add_column['breathing_rate.wav']\n",
    "    raw_br = pd.DataFrame(data = raw_br)\n",
    "    br = 1.0000 * raw_br\n",
    "    br.rename(columns = {0: 'breathing_rate'}, inplace = True)\n",
    "    \n",
    "    #breath rate\n",
    "    t0_br = beginning_time\n",
    "    #br = participant_info_hex['raw_br']\n",
    "    br['Timestamp'] = list(range(0, br.shape[0],1))\n",
    "    br['Timestamp'] = br['Timestamp'].apply(lambda x: x/fs_br+t0_br)\n",
    "    # br['Second'] = br['Timestamp']\n",
    "    br = br.set_index('Timestamp')\n",
    "    # br['Second'] = br['Second'].apply(lambda x: x-br.index[0])\n",
    "    br = br.reset_index()\n",
    "    br.rename(columns={0: 'br'}, inplace=True)\n",
    "\n",
    "\n",
    "    '''ACC processing'''\n",
    "    #raw_accX = wavfile.read(r'/home/maxinehe/Downloads/'+fhex+'/acceleration_X.wav')\n",
    "    #raw_accX = hex_selected_dict[participant][day]['acceleration_X.wav'] #wavfile.read(os.path.join(ecg_folder, subfolder, \"acceleration_X.wav\"))\n",
    "    raw_accX = selected_dict_to_add_column['acceleration_X.wav']\n",
    "    raw_accX = pd.DataFrame(data = raw_accX)\n",
    "    accx = 1.0000 * raw_accX \n",
    "\n",
    "    #raw_accY = wavfile.read(r'/home/maxinehe/Downloads/'+fhex+'/acceleration_Y.wav')\n",
    "    #raw_accY = hex_selected_dict[participant][day]['acceleration_Y.wav'] #wavfile.read(os.path.join(ecg_folder, subfolder, \"acceleration_Y.wav\"))\n",
    "    raw_accY = selected_dict_to_add_column['acceleration_Y.wav']\n",
    "    raw_accY = pd.DataFrame(data = raw_accY)\n",
    "    accy = 1.0000 * raw_accY \n",
    "\n",
    "    #raw_accZ = wavfile.read(r'/home/maxinehe/Downloads/'+fhex+'/acceleration_Z.wav')\n",
    "    #raw_accZ = hex_selected_dict[participant][day]['acceleration_Z.wav'] #wavfile.read(os.path.join(ecg_folder, subfolder, \"acceleration_Z.wav\"))\n",
    "    raw_accZ = selected_dict_to_add_column['acceleration_Z.wav']\n",
    "    raw_accZ = pd.DataFrame(data = raw_accZ)\n",
    "    accz = 1.0000 * raw_accZ \n",
    "\n",
    "    raw_acc = pd.concat([accx, accy, accz], axis=1, ignore_index=True)\n",
    "    raw_acc.rename(columns = {0: 'ACC_X', 1: 'ACC_Y', 2: 'ACC_Z'}, inplace = True)\n",
    "\n",
    "    \n",
    "\n",
    "    #acceleration\n",
    "    t0_acc = beginning_time\n",
    "    #raw_acc = participant_info_hex['raw_acc']\n",
    "    raw_acc['Timestamp'] = list(range(0, raw_acc.shape[0],1))\n",
    "    raw_acc['Timestamp'] = raw_acc['Timestamp'].apply(lambda x: x/fs_acc+t0_acc)\n",
    "    # raw_acc['Second'] = raw_acc['Timestamp']\n",
    "    raw_acc = raw_acc.set_index('Timestamp')\n",
    "    # raw_acc['Second'] = raw_acc['Second'].apply(lambda x: x-raw_acc.index[0])\n",
    "    raw_acc = raw_acc.reset_index()\n",
    "    data_dict = {'processed_ECG':ecg, 'processed_br':br, 'processed_acc':raw_acc}\n",
    "    return(data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0084a4",
   "metadata": {},
   "source": [
    "## Generate Hex data with time and read e4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28342721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hex_selected_dict = hex_data_reading()\n",
    "# e4_data_dict = e4_data_reading(file_type = 'TEMP')\n",
    "# participants = list(hex_selected_dict.keys())\n",
    "# fs_ecg = 256\n",
    "# hex_processed_dict ={}\n",
    "# for participant in participants:\n",
    "#     if len([k for k, v in hex_selected_dict[participant].items() if v])>0: #to get rid of 32\n",
    "#         days = hex_selected_dict[participant].keys()\n",
    "#         hex_processed_dict[participant]={}\n",
    "#         for day in days:\n",
    "#             day_num = day[3]\n",
    "#             if len(e4_data_dict[participant]['day'+day_num+'_e4'].keys())>0:\n",
    "#                 date_info = e4_data_dict[participant]['day'+day_num+'_e4']['TEMP.csv'].columns[0]\n",
    "#                 beginning_time = float(date_info)#/fs_ecg\n",
    "#                 selected_dict_to_add_column = hex_selected_dict[participant][day]\n",
    "#                 if len(selected_dict_to_add_column.keys())== 5:\n",
    "#                     hex_processed_dict[participant][day] = {}\n",
    "#                     #print('participant:',participant, day)\n",
    "#                     hex_processed_dict[participant][day] = add_time_column(beginning_time, selected_dict_to_add_column)\n",
    "            #     else: \n",
    "            #         #print('Sensor signals are not complete:', list(selected_dict_to_add_column.keys()), 'participant: ',participant, 'day'+day_num+'_hex')\n",
    "            #         #print('participant: ',participant, 'day'+day_num+'_hex is empty in hex_selected_dict!')\n",
    "            # else:\n",
    "            #     print('participant: ',participant, 'day'+day_num+'_e4 is empty in e4_data_dict!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c47da",
   "metadata": {},
   "source": [
    "## Syncranization for E4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992724e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index(timestamps, target):\n",
    "\n",
    "    idx = (np.abs(timestamps - target)).argmin()\n",
    "    return idx\n",
    "\n",
    "def internalsync_offset(eda, temp, bvp, acc):\n",
    "    # Extract timestamps to numpy arrays for efficient operations\n",
    "    ts_eda = eda['Timestamp'].values\n",
    "    ts_temp = temp['Timestamp'].values\n",
    "    ts_bvp = bvp['Timestamp'].values\n",
    "    ts_acc = acc['Timestamp'].values\n",
    "    \n",
    "    # Determine the common time frame across all signals\n",
    "    start_time = max(ts_eda[0], ts_temp[0], ts_bvp[0], ts_acc[0])\n",
    "    end_time = min(ts_eda[-1], ts_temp[-1], ts_bvp[-1], ts_acc[-1])\n",
    "    \n",
    "    # Synchronize each signal to the common time frame\n",
    "    offsets = {}\n",
    "    for name, ts in [('eda', ts_eda), ('temp', ts_temp), ('bvp', ts_bvp), ('acc', ts_acc)]:\n",
    "        start_idx = find_closest_index(ts, start_time)\n",
    "        end_idx = find_closest_index(ts, end_time) + 1  # Include the end index in the slice\n",
    "        \n",
    "        if name == 'eda':\n",
    "            offsets['eda'] = eda.iloc[start_idx:end_idx]\n",
    "        elif name == 'temp':\n",
    "            offsets['temp'] = temp.iloc[start_idx:end_idx]\n",
    "        elif name == 'bvp':\n",
    "            offsets['bvp'] = bvp.iloc[start_idx:end_idx]\n",
    "        elif name == 'acc':\n",
    "            offsets['acc'] = acc.iloc[start_idx:end_idx]\n",
    "    \n",
    "    return offsets['eda'], offsets['temp'], offsets['bvp'], offsets['acc']\n",
    "\n",
    "def generate_offset_e4(participant_dict):\n",
    "#     participant_dict = import_e4_csv_files(root_dir =root_dir)\n",
    "    participant_dict_e4_offset = {}\n",
    "#    print('participant_dict: ',participant_dict.keys())\n",
    "    print(\"list(participant_dict.keys(): \",list(participant_dict.keys()))\n",
    "    for j in range(len(list(participant_dict.keys()))):\n",
    "        print('j:', j)\n",
    "        #print('list(participant_dict.keys())[j]: ', list(participant_dict.keys())[j])\n",
    "        selected_key = list(participant_dict.keys())[j]\n",
    "        print(\"selected_key:\", selected_key)\n",
    "        a = participant_dict[selected_key]\n",
    "        print('a: ', a)\n",
    "        print(\"a.keys:\", a.keys())\n",
    "        # offset_eda, offset_temp, offset_bvp, offset_acc_e4 = internalsync_offset(a['EDA'],a['TEMP'],a['BVP'],a['ACC'])\n",
    "        offset_eda, offset_temp, offset_bvp, offset_acc_e4 = internalsync_offset(a['EDA.csv'],a['TEMP.csv'],a['BVP.csv'],a['ACC.csv'])\n",
    "        data_dict = {'offset_eda':offset_eda, 'offset_temp':offset_temp, 'offset_bvp':offset_bvp,\"offset_acc_e4\":offset_acc_e4 }\n",
    "        participant_dict_e4_offset[list(participant_dict.keys())[j]] = data_dict\n",
    "    return(participant_dict_e4_offset)\n",
    "\n",
    "#participant_dict_e4_offset = generate_offset_e4(e4_data_dict[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe606d",
   "metadata": {},
   "source": [
    "## Syncranization for Hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset_hex(participant, hex_processed_dict ):\n",
    "#     participant_dict_HR = import_hex_signals(root_dir =root_dir,fhex = fhex)\n",
    "    participant_dict_hex_offset = {}\n",
    "    for j in range(len(list(hex_processed_dict[participant].keys()))):\n",
    "        a = hex_processed_dict[participant][list(hex_processed_dict[participant].keys())[j]]\n",
    "        \n",
    "        if a['processed_ECG'].shape[0] != 0:\n",
    "            offset_ecg, offset_br, offset_acc_hex, offset_acc_hex = internalsync_offset(a['processed_ECG'],a['processed_br'],a['processed_acc'], a['processed_acc'])\n",
    "            data_dict = {'offset_ecg':offset_ecg, 'offset_br':offset_br, 'offset_acc_hex':offset_acc_hex}\n",
    "            participant_dict_hex_offset[list(hex_processed_dict[participant].keys())[j]] = data_dict\n",
    "        else: \n",
    "            print(participant, list(hex_processed_dict[participant].keys())[j], 'ECG data is empty!')\n",
    "    return(participant_dict_hex_offset)\n",
    "\n",
    "\n",
    "# # Constants\n",
    "# sampling_rate = 256  # Hz\n",
    "# duration_per_split = 30 #min\n",
    "# participant_dict_hex_offset ={}\n",
    "# for participant in hex_processed_dict.keys():\n",
    "#     #print('participant: ', participant)\n",
    "#     participant_dict_hex_offset[participant] = generate_offset_hex(participant, hex_processed_dict=hex_processed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02271409",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 256  # Hz\n",
    "duration_per_split = 5 #min\n",
    "participant_dict_hex_offset ={}\n",
    "\n",
    "hex_selected_dict = hex_data_reading()\n",
    "e4_data_dict = e4_data_reading(file_type = 'TEMP')\n",
    "participants = list(hex_selected_dict.keys())\n",
    "fs_ecg = 256\n",
    "hex_processed_dict ={}\n",
    "for participant in participants:\n",
    "    if len([k for k, v in hex_selected_dict[participant].items() if v])>0: #to get rid of 32\n",
    "        days = hex_selected_dict[participant].keys()\n",
    "        hex_processed_dict[participant]={}\n",
    "        for day in days:\n",
    "            day_num = day[3]\n",
    "            if len(e4_data_dict[participant]['day'+day_num+'_e4'].keys())>0:\n",
    "                date_info = e4_data_dict[participant]['day'+day_num+'_e4']['TEMP.csv'].columns[0]\n",
    "                beginning_time = float(date_info)#/fs_ecg\n",
    "                selected_dict_to_add_column = hex_selected_dict[participant][day]\n",
    "                if len(selected_dict_to_add_column.keys())== 5:\n",
    "                    hex_processed_dict[participant][day] = {}\n",
    "                    #print('participant:',participant, day)\n",
    "                    hex_processed_dict[participant][day] = add_time_column(beginning_time, selected_dict_to_add_column)\n",
    "        print(participant)\n",
    "        participant_dict_hex_offset[participant] = generate_offset_hex(participant, hex_processed_dict)\n",
    "\n",
    "# for participant in hex_processed_dict.keys():\n",
    "#     #print('participant: ', participant)\n",
    "#     participant_dict_hex_offset[participant] = generate_offset_hex(participant, hex_processed_dict=hex_processed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickles(participant_dict_hex_offset):\n",
    "\n",
    "\n",
    "    # Save all_split_dict\n",
    "    with open('participant_dict_hex_offset.pkl', 'wb') as f:\n",
    "        pickle.dump(participant_dict_hex_offset, f)\n",
    "\n",
    "    \n",
    "    hex_ECG_dict_path = os.path.abspath('participant_dict_hex_offset.pkl')\n",
    "    #label_dict_path = os.path.abspath('all_label_dict_'+name+'_min.pkl)\n",
    "    print(\"The pickle files are saved in:\\n \", hex_ECG_dict_path)\n",
    "\n",
    "#save_pickles(participant_dict_hex_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hex_ecg_dict(offset_dict, signal_key='offset_ecg'):\n",
    "    \"\"\"\n",
    "    Extract the desired signal (default = 'offset_ecg') from a\n",
    "    participant_dict_hex_offset–style structure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    offset_dict : dict\n",
    "        Mapping      participant_id → day_key → {signal_key, …}\n",
    "    signal_key   : str, optional\n",
    "        The key whose value you want to keep (default 'offset_ecg').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hex_ecg_dict : dict\n",
    "        Same two‑level layout (participant → day), but each day maps\n",
    "        directly to the signal object (e.g. a NumPy array or DataFrame)\n",
    "        instead of the whole modality bundle.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        pid: {\n",
    "            day: day_bundle[signal_key]\n",
    "            for day, day_bundle in day_map.items()\n",
    "            if signal_key in day_bundle          # skip if missing\n",
    "        }\n",
    "        for pid, day_map in offset_dict.items()\n",
    "    }\n",
    "\n",
    "# ---- usage ----------------------------------------------------------\n",
    "hex_ECG_dict = build_hex_ecg_dict(participant_dict_hex_offset)\n",
    "\n",
    "# # quick sanity‑check\n",
    "# print(hex_ECG_dict.keys())                     # participants\n",
    "# print(hex_ECG_dict[33].keys())                 # day0_hex … day9_hex\n",
    "#      # whatever 'offset_ecg' holds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pickles(hex_ECG_dict):\n",
    "\n",
    "\n",
    "#     # Save all_split_dict\n",
    "#     with open('hex_ECG_dict.pkl', 'wb') as f:\n",
    "#         pickle.dump(hex_ECG_dict, f)\n",
    "\n",
    "    \n",
    "#     hex_ECG_dict_path = os.path.abspath('hex_ECG_dict.pkl')\n",
    "#     #label_dict_path = os.path.abspath('all_label_dict_'+name+'_min.pkl)\n",
    "#     print(\"The pickle files are saved in:\\n \", hex_ECG_dict_path)\n",
    "\n",
    "# save_pickles(hex_ECG_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_split_dict(patients_dayly_tags, sampling_rate = sampling_rate, duration_per_split = duration_per_split):\n",
    "    duration_per_split = duration_per_split * 60  # 30 minutes in seconds\n",
    "    rows_per_split = int(duration_per_split * sampling_rate)  # Total rows for 30 minutes\n",
    "\n",
    "    # Initialize the dictionary to store splits\n",
    "    split_dict = {}\n",
    "\n",
    "    # Split the dataframe into chunks of 30 minutes\n",
    "    for i in range(math.ceil(len(patients_dayly_tags) / rows_per_split)):\n",
    "        start_index = i * rows_per_split\n",
    "        end_index = start_index + rows_per_split\n",
    "        split_patients_dayly_tags = patients_dayly_tags.iloc[start_index:end_index]\n",
    "        \n",
    "        # Use the starting time of the split as the dictionary key\n",
    "        split_start_time = split_patients_dayly_tags['Timestamp'].iloc[0]\n",
    "        split_dict[split_start_time] = split_patients_dayly_tags.drop(columns=['Timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # Example: Access one of the dictionaries\n",
    "    # print(\"Number of splits:\", len(split_dict))\n",
    "    # print(\"Sample split key:\", list(split_dict.keys())[0])\n",
    "    # print(\"Sample split dataframe:\")\n",
    "    # print(split_dict[list(split_dict.keys())[0]])\n",
    "    return split_dict\n",
    "# split_dict = generate_split_dict(patients_dayly_tags)\n",
    "\n",
    "def generate_label_dict(tags,split_dict,duration_per_split = duration_per_split ):\n",
    "\n",
    "    # Initialize label dictionary\n",
    "    label_dict = {}\n",
    "\n",
    "        # If tags length is 0, set all labels to 0\n",
    "    if len(tags) == 0:\n",
    "        for split_start_time in split_dict.keys():\n",
    "            label_dict[split_start_time] = 0\n",
    "        return label_dict\n",
    "\n",
    "    # Create labels for each split in the split dictionary\n",
    "    for split_start_time in split_dict.keys():\n",
    "        # Calculate the 30-minute threshold (in seconds)\n",
    "        split_end_time = split_start_time + duration_per_split * 60\n",
    "        \n",
    "        # Check if any tag falls within the 30-minute window of the current split\n",
    "        label = int(any(tag >= split_start_time and tag < split_end_time for tag in tags.values.reshape(-1)))\n",
    "\n",
    "        \n",
    "        # Assign the label to the split\n",
    "        label_dict[split_start_time] = label\n",
    "    return label_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64314e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming e4_data_dict is defined\n",
    "def count_the_tags(e4_data_dict):\n",
    "    e4_tags_dict = {}\n",
    "    for key, directory in e4_data_dict.items():\n",
    "        e4_tags_dict[key] = {}\n",
    "        for day, read_files in directory.items():\n",
    "            e4_tags_dict[key][day] = {}\n",
    "            read_files_list = list(read_files)\n",
    "            if 'tags.csv' in read_files_list:\n",
    "                #print('key:', key, 'day:', day, 'is tagged.')\n",
    "                e4_tags_dict[key][day] = e4_data_dict[key][day]['tags.csv']\n",
    "            else:\n",
    "                #print('key:', key, 'day:', day, 'is empty.')\n",
    "                e4_tags_dict[key][day] = {}\n",
    "    return(e4_tags_dict)\n",
    "\n",
    "def generate_tag_counts_dict(e4_tags_dict):\n",
    "    tag_counts = {}\n",
    "\n",
    "    for key, days in e4_tags_dict.items():\n",
    "        tag_counts[key] = {}\n",
    "        for day, tags in days.items():\n",
    "            if isinstance(tags, pd.DataFrame):  # Check if tags is a DataFrame\n",
    "                tag_counts[key][day] = len(tags) if not tags.empty else 0\n",
    "            else:\n",
    "                tag_counts[key][day] = 0  # No tags or not a DataFrame\n",
    "    # Finding keys with no tagging (all tag counts are 0)\n",
    "    no_tagging_keys = [key for key, days in tag_counts.items() if all(count == 0 for count in days.values())]\n",
    "\n",
    "    print(\"no_tagging_keys: {}\".format(no_tagging_keys))\n",
    "    return(tag_counts)\n",
    "\n",
    "\n",
    "def generate_patients_dayly_tags_from_tag_counts(tag_counts):\n",
    "    data = []\n",
    "    for key, days in tag_counts.items():\n",
    "        for day, count in days.items():\n",
    "            data.append({'Key': key, 'Day': day, 'Tags': count})\n",
    "\n",
    "    patients_dayly_tags = pd.DataFrame(data)\n",
    "    return(patients_dayly_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0852a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pickle_for_data_dict(hex_ECG_dict,e4_tags_dict, sampling_rate = sampling_rate,duration_per_split =duration_per_split):\n",
    "    all_split_dict = {}\n",
    "    all_label_dict = {}\n",
    "    for key in list(hex_ECG_dict.keys()):\n",
    "        print('key: ', key)\n",
    "        all_split_dict[key] = {}\n",
    "        all_label_dict[key] = {}\n",
    "        for day in hex_ECG_dict[key]:\n",
    "            #print(day)\n",
    "            if len(hex_ECG_dict[key][day])>0:\n",
    "                #print('satisfied')\n",
    "                #patients_dayly_tags[patients_dayly_tags['Key']==key]\n",
    "                todays_tags = hex_ECG_dict[key][day]\n",
    "                split_dict = generate_split_dict(todays_tags,sampling_rate = sampling_rate, duration_per_split = duration_per_split)\n",
    "                #split_dict = generate_split_dict(patients_dayly_tags, sampling_rate = sampling_rate, duration_per_split = duration_per_split)\n",
    "                day_num = day[3]\n",
    "                tags = e4_tags_dict[key]['day'+day_num+'_e4']  \n",
    "                label_dict = generate_label_dict(tags,split_dict,duration_per_split = duration_per_split )\n",
    "                all_split_dict[key][day]=split_dict\n",
    "                all_label_dict[key][day]=label_dict\n",
    "            else:\n",
    "                print(f\"For participant {key} day {day} is empty!\")\n",
    "    return(all_split_dict,all_label_dict )\n",
    "\n",
    "\n",
    "#e4_data_dict = e4_data_reading(file_type = 'TEMP')\n",
    "e4_tags_dict = count_the_tags(e4_data_dict)\n",
    "# tag_counts =generate_tag_counts_dict(e4_tags_dict)\n",
    "# patients_dayly_tags = generate_patients_dayly_tags_from_tag_counts(tag_counts)\n",
    "\n",
    "your_keys = [33, 8]\n",
    "border = 5\n",
    "#name_border='first_'+str(border)\n",
    "your_keys = list(hex_ECG_dict.keys()) #[:border]\n",
    "dict_you_want = {key: hex_ECG_dict[key] for key in your_keys}\n",
    "all_split_dict,all_label_dict = generate_pickle_for_data_dict(dict_you_want,e4_tags_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pickles(all_split_dict, all_label_dict, duration_per_split):\n",
    "#     name = str(duration_per_split)\n",
    "\n",
    "#     # Save all_split_dict\n",
    "#     with open('all_split_ECG_dict_'+name+'_min.pkl', 'wb') as f:\n",
    "#         pickle.dump(all_split_dict, f)\n",
    "\n",
    "#     # Save all_label_dict\n",
    "#     with open('all_label_ECG_dict_'+name+'_min.pkl', 'wb') as f:\n",
    "#         pickle.dump(all_label_dict, f)\n",
    "    \n",
    "#     split_dict_path = os.path.abspath('all_split_ECG_dict_'+name+'_min.pkl')\n",
    "#     label_dict_path = os.path.abspath('all_label_dict_'+name+'_min.pkl')\n",
    "#     print(\"The pickle files are saved in:\\n \", split_dict_path)\n",
    "\n",
    "# save_pickles(all_split_dict, all_label_dict, duration_per_split)\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_pickles(all_split_dict, all_label_dict, duration_per_split,\n",
    "                 dest_dir=\"/work/hdd/bbnp/wear_raw\"):\n",
    "    name = str(duration_per_split)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Build file paths\n",
    "    split_filename = f\"all_split_ECG_dict_{name}_min.pkl\"\n",
    "    label_filename = f\"all_label_ECG_dict_{name}_min.pkl\"\n",
    "    split_path = os.path.join(dest_dir, split_filename)\n",
    "    label_path = os.path.join(dest_dir, label_filename)\n",
    "\n",
    "    # Save\n",
    "    with open(split_path, 'wb') as f:\n",
    "        pickle.dump(all_split_dict, f)\n",
    "    with open(label_path, 'wb') as f:\n",
    "        pickle.dump(all_label_dict, f)\n",
    "\n",
    "    print(\"Pickle files saved to:\")\n",
    "    print(\"  \", split_path)\n",
    "    print(\"  \", label_path)\n",
    "\n",
    "# Usage:\n",
    "save_pickles(all_split_dict, all_label_dict, duration_per_split)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
